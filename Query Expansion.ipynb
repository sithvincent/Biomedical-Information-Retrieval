{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval of Article IDs from Natural Language Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code here has been abstracted away into pubmed_search, thus all are commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import spacy\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Load SpaCy's language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Define stop words using NLTK\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(query):\n",
    "#     doc = nlp(query)\n",
    "#     # Filter out stop words and only keep nouns and adjectives\n",
    "#     filtered_tokens = [token.text for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"] and token.text.lower() not in stop_words]\n",
    "#     return \" \".join(filtered_tokens)\n",
    "\n",
    "# def get_query_response(query, preprocess = True, articles_to_retrieve = 5):\n",
    "#     if preprocess:\n",
    "#         query = remove_stop_words(query)\n",
    "#         print('Preprocessed query:', query)\n",
    "#     base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "#     params = {\n",
    "#     \"db\": \"pubmed\",        # Database to search\n",
    "#     \"term\": query,  # Your natural language query\n",
    "#     \"retmax\": articles_to_retrieve,         # Number of results to return\n",
    "#     'api_key' : '2016449eab49266b3ccf1be9ba0c52b8d809',\n",
    "#     \"retmode\": \"json\" ,     # Response format    \n",
    "#     'maxdate':\"2022/12/30\"\n",
    "#     }\n",
    "#     response = requests.get(base_url, params=params)\n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the JSON response\n",
    "#         data = response.json()\n",
    "#     else:\n",
    "#         print(f\"Error: {response.status_code}\")\n",
    "#     return data\n",
    "\n",
    "# # response = requests.get(url= 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=science[journal]+AND+breast+cancer+AND+2008[pdat]&retmode=json&api_key=2016449eab49266b3ccf1be9ba0c52b8d809')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next methods extract article details from extracted ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parsing the XML response\n",
    "# from xml.etree import ElementTree as ET\n",
    "\n",
    "# def get_article_details_from_id(id_list, important_headings_only = True):\n",
    "#     efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "#     efetch_params = {\n",
    "#         \"db\": \"pubmed\",\n",
    "#         \"id\": \",\".join(id_list),  # Join PMIDs with commas\n",
    "#         \"retmode\": \"xml\",               # Request XML format for easier parsing\n",
    "#         \"rettype\": \"abstract\"           # Request abstract type\n",
    "#     }\n",
    "\n",
    "#     # Send the EFetch request\n",
    "#     efetch_response = requests.get(efetch_url, params=efetch_params)\n",
    "\n",
    "#     if efetch_response.status_code == 200:\n",
    "#         root = ET.fromstring(efetch_response.content)\n",
    "#         articles = root.findall(\".//PubmedArticle\")\n",
    "\n",
    "#         # # Original extraction\n",
    "#         # retrieved_abstracts = {}\n",
    "#         # for article in articles:\n",
    "#         #     pmid = article.findtext(\".//PMID\")\n",
    "#         #     title = article.findtext(\".//ArticleTitle\")\n",
    "#         #     abstract = article.findtext(\".//AbstractText\")\n",
    "#         #     retrieved_abstracts[pmid] = {\n",
    "#         #         'title': title,\n",
    "#         #         'abstract': abstract\n",
    "#         #     }\n",
    "#         #     print(f\"PMID: {pmid}\\nTitle: {title}\\nAbstract: {abstract}\\n{'-'*80}\\n\")        \n",
    "#         # return retrieved_abstracts\n",
    "\n",
    "#         # Improved extraction\n",
    "#         retrieved_data = {}\n",
    "#         for article in articles:\n",
    "#             pmid = article.findtext(\".//PMID\")\n",
    "#             title = article.findtext(\".//ArticleTitle\")\n",
    "#             abstract = article.findtext(\".//AbstractText\")            \n",
    "#             # Extract all MeSH Headings and details for each article\n",
    "#             mesh_headings = []\n",
    "#             for mesh_heading in article.findall(\".//MeshHeading/DescriptorName\"):\n",
    "#                 descriptor_name = mesh_heading.text\n",
    "#                 descriptor_ui = mesh_heading.attrib['UI']     \n",
    "#                 descriptor_major_topic = mesh_heading.attrib.get('MajorTopicYN', 'Not Found')\n",
    "#                 # This one will extract only major article descriptors if important_headings_only is set to True\n",
    "#                 if (important_headings_only and (descriptor_major_topic=='Y')) or not important_headings_only:\n",
    "#                     mesh_heading_details = {\n",
    "#                         'Descriptor': descriptor_name,\n",
    "#                         'Descriptor UI': descriptor_ui,\n",
    "#                         'Is Major Topic': descriptor_major_topic\n",
    "#                     }\n",
    "#                     mesh_headings.append(mesh_heading_details)\n",
    "            \n",
    "#             retrieved_data[pmid] = {\n",
    "#                 'Title': title,\n",
    "#                 'Abstract': abstract,\n",
    "#                 'Mesh Headings': mesh_headings\n",
    "#             }\n",
    "#             # print(f\"PMID: {pmid}\\nTitle: {title}\\nAbstract: {abstract}\\nMeSH Headings: {', '.join(mesh_headings)}\\n{'-'*80}\\n\")\n",
    "#         return retrieved_data\n",
    "#     else:\n",
    "#         print(f\"Error fetching articles: {efetch_response.status_code}\")\n",
    "#         return None\n",
    "\n",
    "# # Run the function on extracted IDs (ideally, store in in vector DB and rank the retrieval)\n",
    "# retrieved_article_details = get_article_details_from_id(retrieved_ids)\n",
    "# retrieved_article_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses functions abstracted from the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import helper.pubmed_search as pubs\n",
    "\n",
    "# with open('helper/descriptors.json') as json_file:\n",
    "#     descriptors = json.load(json_file)\n",
    "# with open('helper/concepts.json') as json_file:\n",
    "#     concepts = json.load(json_file)\n",
    "# with open('helper/terms.json') as json_file:\n",
    "#     terms = json.load(json_file)\n",
    "\n",
    "# query = 'I have purple rashes and cough'\n",
    "# # query_response = pubs.get_query_response(query)\n",
    "# retrieved_article_details, translated_query = pubs.extract_article_details_from_query(query, articles_to_retrieve=20, remove_stop_words=False)\n",
    "# retrieved_article_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Query Suggestion (With Haystack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we used Haystack but now no longer - just straight away obtain embeddings to compare via Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper.LLM_retrieval import RetrievalManager\n",
    "# from helper.config import api_key\n",
    "# from haystack import Document\n",
    "# import pandas as pd\n",
    "\n",
    "# openai_retrieval_manager = RetrievalManager(api_key)\n",
    "# retrieval_manager = RetrievalManager(api_key, embedding_model='open source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First level: Term suggestion directly from semantic similarity with extracted articles' MeSH headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heading_entries = []\n",
    "# for key, value in retrieved_article_details.items():\n",
    "#     # Extract and embed all extracted headings\n",
    "#     for mesh_heading in value['Mesh Headings']:\n",
    "#         heading_entries.append(Document(content=descriptors[mesh_heading['Descriptor']]['Combined Description'], meta={'file_path': mesh_heading['Descriptor'], 'file_type': 'Headings'}))\n",
    "#     title = value['Title'] or ''\n",
    "#     abstract = value ['Abstract'] or ''\n",
    "#     title_abstract = title + abstract\n",
    "#     heading_entries.append(Document(content=title_abstract, meta={'file_path': key, 'file_type': 'Title and Abstract'}))\n",
    "\n",
    "# retrieval_manager.read_individual_documents(heading_entries)\n",
    "# openai_retrieval_manager.read_individual_documents(heading_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for document purging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def purge_document_store(document_store):\n",
    "#     list_of_ids = []\n",
    "#     for document in document_store.filter_documents({\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Headings\"}):\n",
    "#         list_of_ids.append(document.to_dict()['id'])\n",
    "#     document_store.delete_documents(list_of_ids)\n",
    "#     # print('Post deletion document store size:', document_store.count_documents())\n",
    "\n",
    "# print('Pre deletion document store size:', headings_retrieval_manager.document_store.count_documents())\n",
    "# purge_document_store(headings_retrieval_manager.document_store)\n",
    "# print('Post deletion document store size:', headings_retrieval_manager.document_store.count_documents())\n",
    "# # headings_retrieval_manager.document_store.filter_documents({\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Headings\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for non openai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = retrieval_manager.answer_user(query, filter={\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Headings\"})['retriever_selected_candidates']\n",
    "# # answer = retrieval_manager.answer_user(query)['retriever_selected_candidates']\n",
    "# description_selected_descriptors = []\n",
    "# for key, value in answer.items():\n",
    "#     if value >= heading_threshold:\n",
    "#         temp_dict = {\n",
    "#             'Name': key,\n",
    "#             'Description': descriptors[key]['Combined Description'],\n",
    "#             'Suitability': value*100\n",
    "#         }\n",
    "#         description_selected_descriptors.append(temp_dict)\n",
    "# headings_for_query_suggestion = pd.DataFrame(description_selected_descriptors)\n",
    "# headings_for_query_suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For openai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heading_threshold = 0.1\n",
    "# answer = openai_retrieval_manager.answer_user(query, filter={\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Headings\"})['retriever_selected_candidates']\n",
    "# # answer = retrieval_manager.answer_user(query)['retriever_selected_candidates']\n",
    "# description_selected_descriptors = []\n",
    "# for key, value in answer.items():\n",
    "#     if value >= heading_threshold:\n",
    "#         temp_dict = {\n",
    "#             'Name': key,\n",
    "#             'Description': descriptors[key]['Combined Description'],\n",
    "#             'Suitability': value*100\n",
    "#         }\n",
    "#         description_selected_descriptors.append(temp_dict)\n",
    "# headings_for_query_suggestion = pd.DataFrame(description_selected_descriptors)\n",
    "# headings_for_query_suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term suggestion from semantic similarity with MeSH headings from most relevant articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = retrieval_manager.answer_user(query, filter={\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Title and Abstract\"})['retriever_selected_candidates']\n",
    "# # answer = retrieval_manager.answer_user(query)['retriever_selected_candidates']\n",
    "# shortlisted_articles = []\n",
    "# for key, value in answer.items():\n",
    "#     if value >= title_abstract_threshold:\n",
    "#         temp_dict = {\n",
    "#             'Name': key,\n",
    "#             'Title': retrieved_article_details[key]['Title'],\n",
    "#             # 'Abstract': retrieved_article_details[key]['Abstract'],\n",
    "#             'Suitability': value*100\n",
    "#         }\n",
    "#         shortlisted_articles.append(temp_dict)\n",
    "# articles_for_query_suggestion = pd.DataFrame(shortlisted_articles)\n",
    "# articles_for_query_suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the openai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = openai_retrieval_manager.answer_user(query, filter={\"field\": \"meta.file_type\", \"operator\": \"==\", \"value\": \"Title and Abstract\"})['retriever_selected_candidates']\n",
    "# # answer = retrieval_manager.answer_user(query)['retriever_selected_candidates']\n",
    "# title_abstract_threshold = 0.3\n",
    "# shortlisted_articles = []\n",
    "# for key, value in answer.items():\n",
    "#     if value >= title_abstract_threshold:\n",
    "#         temp_dict = {\n",
    "#             'Name': key,\n",
    "#             'Title': retrieved_article_details[key]['Title'],\n",
    "#             # 'Abstract': retrieved_article_details[key]['Abstract'],\n",
    "#             'Suitability': value*100\n",
    "#         }\n",
    "#         shortlisted_articles.append(temp_dict)\n",
    "# articles_for_query_suggestion = pd.DataFrame(shortlisted_articles)\n",
    "# articles_for_query_suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, extract out the terms we need for query suggestion and append it to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # headings_from_articles_for_query_suggestion = \n",
    "# for shortlisted_article in shortlisted_articles:\n",
    "#     article_related_headings = retrieved_article_details[shortlisted_article['Name']]['Mesh Headings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Query Suggestion (With Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# from openai import OpenAI\n",
    "# from helper.config import api_key\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# def embed_mesh_headings(text, model='openai', api_key = ''):\n",
    "#     '''Takes in a text and desired model as argument and outputs the embedding vector.'''\n",
    "#     if model == 'openai':\n",
    "#         client = OpenAI(api_key = api_key)\n",
    "#         text = text.replace(\"\\n\", \" \")\n",
    "#         embeddings = np.array([client.embeddings.create(input = [text], model=model).data[0].embedding])\n",
    "#     else:\n",
    "#         model_name = model\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model = AutoModel.from_pretrained(model_name)\n",
    "#         # Tokenize the text and get embeddings\n",
    "#         inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = model(**inputs).last_hidden_state.mean(dim=1).numpy()  # Use mean pooling for sentence embedding\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# def embed_mesh_headings_preloaded_model(text, **kwargs):\n",
    "#     '''Takes in a text and desired model as argument and outputs the embedding vector.\n",
    "#     For each model, the corresponding details would also need to be passed in where necessary.'''\n",
    "#     if 'client' not in kwargs: #if not an openai model\n",
    "#         tokenizer = kwargs['tokenizer']\n",
    "#         model = kwargs['model']\n",
    "#         # Tokenize the text and get embeddings\n",
    "#         inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = model(**inputs).last_hidden_state.mean(dim=1).numpy()  # Use mean pooling for sentence embedding    \n",
    "#     else:\n",
    "#         text = text.replace(\"\\n\", \" \")\n",
    "#         embeddings = np.array([kwargs['client'].embeddings.create(input = [text], model=\"text-embedding-3-large\").data[0].embedding])\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize models and test whether the models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key = api_key)\n",
    "# # model_name = \"w601sxs/b1ade-embed\"\n",
    "# # model_name = \"dmis-lab/biobert-v1.1\"  \n",
    "# model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# text1 = \"This is the first sentence.\"\n",
    "# text2 = \"This is another sentence.\"\n",
    "# # # Old method that initializes the model inside the function\n",
    "# # cosine_sim = cosine_similarity(embed_mesh_headings(text1, \"w601sxs/b1ade-embed\" ), embed_mesh_headings(text2, \"w601sxs/b1ade-embed\" ))\n",
    "# # cosine_sim[0][0]\n",
    "\n",
    "# # Proper method (pass model in as an argument)\n",
    "# cosine_sim = cosine_similarity(embed_mesh_headings_preloaded_model(text1, tokenizer= tokenizer, model= model), \n",
    "#                                embed_mesh_headings_preloaded_model(text2, tokenizer= tokenizer, model= model))\n",
    "# cosine_sim[0][0]\n",
    "\n",
    "# # Proper method (pass model in as an argument)\n",
    "# cosine_sim = cosine_similarity(embed_mesh_headings_preloaded_model(text1, client= client), \n",
    "#                                embed_mesh_headings_preloaded_model(text2, client= client))\n",
    "# cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original semantic similarity check functions (obsolete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storing the embeddings all of the details returned from the search\n",
    "# # NEED TO FURTHER DECOUPLE AND MODULARIZE\n",
    "# def extract_content_and_embeddings_from_retrieved_article_details(retrieved_article_details):\n",
    "#     heading_entries = []\n",
    "#     article_entries = []\n",
    "#     for key, value in retrieved_article_details.items():\n",
    "#         # Extract and embed all extracted headings\n",
    "#         for mesh_heading in value['Mesh Headings']:\n",
    "#             heading_entry = {\n",
    "#                 'name': mesh_heading['Descriptor'],\n",
    "#                 'content': descriptors[mesh_heading['Descriptor']]['Combined Description'],\n",
    "#                 'embeddings': embed_mesh_headings_preloaded_model(descriptors[mesh_heading['Descriptor']]['Combined Description'], tokenizer= tokenizer, model= model)\n",
    "#             }\n",
    "#             heading_entries.append(heading_entry)\n",
    "#         title = value['Title'] or ''\n",
    "#         abstract = value ['Abstract'] or ''\n",
    "#         title_abstract = title + abstract\n",
    "#         article_entry = {\n",
    "#             'name': key,\n",
    "#             'content': title_abstract,\n",
    "#             'embeddings': embed_mesh_headings_preloaded_model(title_abstract, tokenizer= tokenizer, model= model)\n",
    "#         }\n",
    "#         article_entries.append(article_entry)\n",
    "#     return heading_entries, article_entries\n",
    "\n",
    "# heading_entries, article_entries = extract_content_and_embeddings_from_retrieved_article_details(retrieved_article_details)\n",
    "\n",
    "# # Compare vector similarity of query to these items\n",
    "# def extract_filtered_entries_from_content_and_embeddings(heading_entries, article_entries):\n",
    "#     filtered_heading_entries = []\n",
    "#     filtered_article_entries = []\n",
    "#     query_embedding = embed_mesh_headings_preloaded_model(query, tokenizer= tokenizer, model= model)\n",
    "#     for heading_entry in heading_entries:\n",
    "#         similarity = cosine_similarity(query_embedding, heading_entry['embeddings'])[0][0]\n",
    "#         print('Heading similarity is:', similarity)\n",
    "#         # if  similarity > heading_threshold:\n",
    "#         if similarity > heading_threshold:\n",
    "#             heading_entry['Suitability'] = similarity*100\n",
    "#             filtered_heading_entries.append(heading_entry)\n",
    "#     for article_entry in article_entries:\n",
    "#         similarity = cosine_similarity(query_embedding, article_entry['embeddings'])[0][0]\n",
    "#         print('Article similarity is:', similarity)\n",
    "#         # if similarity > title_abstract_threshold:\n",
    "#         if similarity > title_abstract_threshold:\n",
    "#             article_entry['Suitability'] = similarity*100\n",
    "#             filtered_article_entries.append(article_entry)\n",
    "#     return filtered_heading_entries, filtered_article_entries\n",
    "\n",
    "# filtered_heading_entries, filtered_article_entries = extract_filtered_entries_from_content_and_embeddings(heading_entries, article_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new manager class that supersedes previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DataMicron\\anaconda3\\envs\\nlpenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# class QueryExpansionManager():\n",
    "#     def __init__(self, model_name, descriptor_json_location, api_key=None,) -> None:\n",
    "#         if model_name == 'openai':\n",
    "#             if api_key is None:\n",
    "#                 raise ValueError(\"API key is required for OpenAI models\")\n",
    "#             self.model_name = model_name\n",
    "#             self.client = OpenAI(api_key = api_key)\n",
    "#             self.tokenizer = None\n",
    "#             self.model = None\n",
    "#         else:\n",
    "#             self.model_name = model_name\n",
    "#             self.client = None\n",
    "#             self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#             self.model = AutoModel.from_pretrained(model_name)\n",
    "#         with open(descriptor_json_location) as json_file:\n",
    "#             self.descriptors = json.load(json_file)\n",
    "#         self.query = ''\n",
    "#         self.translated_query=''\n",
    "#         self.retrieved_article_details = {}    \n",
    "\n",
    "#     # This function handles embedding for all the rest of the task in this class \n",
    "#     # It is crucial in standardizing the Manager class regardless of embedding models since it returns a unified format that can be used by all other functions.\n",
    "#     def embed_mesh_headings_preloaded_model(self, text):\n",
    "#         '''Takes in a text as argument and outputs the embedding vector.'''\n",
    "#         #if using an openai model\n",
    "#         if self.model_name == 'openai':\n",
    "#             text = text.replace(\"\\n\", \" \")\n",
    "#             embeddings = np.array([self.client.embeddings.create(input = [text], model=\"text-embedding-3-large\").data[0].embedding])\n",
    "#         else: \n",
    "#             # Tokenize the text and get embeddings\n",
    "#             inputs = self.tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             with torch.no_grad():\n",
    "#                 embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).numpy()  # Use mean pooling for sentence embedding            \n",
    "#         return embeddings    \n",
    "    \n",
    "#     # This function will extract\n",
    "#     def extract_filtered_entries_from_retrieved_article_details(self, retrieved_article_details, query, title_abstract_threshold, heading_threshold):\n",
    "#         '''Takes in several arguments and returns the relevant heading entries and article entries, in that order.'''\n",
    "#         self.retrieved_article_details = retrieved_article_details\n",
    "#         self.query = query\n",
    "#         query_embedding = self.embed_mesh_headings_preloaded_model(query)\n",
    "#         heading_entries = []\n",
    "#         article_entries = []\n",
    "#         for key, value in retrieved_article_details.items():\n",
    "#             article_headings = []\n",
    "#             # Extract and embed all extracted headings for separate relevance ranking of the terms itself\n",
    "#             for mesh_heading in value['Mesh Headings']: \n",
    "#                 article_headings.append(mesh_heading['Descriptor'])\n",
    "#                 mesh_heading_embeddings = self.embed_mesh_headings_preloaded_model(descriptors[mesh_heading['Descriptor']]['Combined Description'])   \n",
    "#                 similarity = cosine_similarity(query_embedding, mesh_heading_embeddings)[0][0]    # output is in fraction\n",
    "#                 if similarity > heading_threshold:        \n",
    "#                     heading_entry = {\n",
    "#                         'name': mesh_heading['Descriptor'],\n",
    "#                         'content': descriptors[mesh_heading['Descriptor']]['Combined Description'],\n",
    "#                         'embeddings': mesh_heading_embeddings,\n",
    "#                         'suitability': similarity*100\n",
    "#                     }\n",
    "#                     heading_entries.append(heading_entry)\n",
    "#             title = value['Title'] or ''\n",
    "#             abstract = value ['Abstract'] or ''\n",
    "#             title_abstract = title + abstract\n",
    "#             article_title_abstract_embeddings = self.embed_mesh_headings_preloaded_model(title_abstract)\n",
    "#             article_similarity = cosine_similarity(query_embedding, article_title_abstract_embeddings)[0][0]\n",
    "#             if article_similarity > title_abstract_threshold:\n",
    "#                 article_entry = {\n",
    "#                     'name': key,\n",
    "#                     'content': title_abstract,\n",
    "#                     'embeddings': article_title_abstract_embeddings,\n",
    "#                     'headings': article_headings,\n",
    "#                     'suitability': similarity*100\n",
    "#                 }\n",
    "#                 article_entries.append(article_entry)\n",
    "#         return heading_entries, article_entries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This subsection will extract out relevant articles and mesh headings.\n",
    "\n",
    "Will also extract out the relevant MeSH keywords (using either the set method that aggregates all articles or are relevant to just one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.pubmed_search import QueryExpansionManager\n",
    "from helper.config import api_key\n",
    "import pandas as pd\n",
    "\n",
    "model_name1 = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_name2 = \"w601sxs/b1ade-embed\"  \n",
    "# mpnet_qe_manager = QueryExpansionManager(model_name1, 'helper/descriptors.json')\n",
    "blade_embed_qe_manager = QueryExpansionManager(model_name2, 'helper/descriptors.json')\n",
    "# openai_qe_manager = QueryExpansionManager('openai', 'helper/descriptors.json', api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is creatinine assessment included in the MELD score?\"\n",
    "# mpnet_qe_manager.get_retrieved_article_details_from_query(query)\n",
    "blade_embed_qe_manager.get_retrieved_article_details_from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_abstract_threshold = 0.3\n",
    "heading_threshold = 0.5\n",
    "heading_entries, article_entries = blade_embed_qe_manager.get_filtered_entries_from_retrieved_article_details(title_abstract_threshold, heading_threshold)\n",
    "if heading_entries!=[]:\n",
    "    heading_entries_df = pd.DataFrame(heading_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)\n",
    "else:\n",
    "    print('Nothing returned!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each article within the relevance threshold, we extract out the concepts relevant to it (optionally rank the descriptors based on relevance but no need) and then link them together using 'AND' and enclose them in one bracket as an 'article concept' in a semantic region/viscinity/cluster. In turn, each article is linked to other articles with an 'OR'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AND \"Biomarkers\"[MeSH Terms] AND \"Severity of Illness Index\"[MeSH Terms] ) OR (\"Amino Acids, Branched-Chain\"[MeSH Terms] AND \"Dietary Supplements\"[MeSH Terms] AND \"Muscle Strength\"[MeSH Terms] ) OR (\"Acute Kidney Injury\"[MeSH Terms] ) OR AND \"Nomograms\"[MeSH Terms] )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = blade_embed_qe_manager.create_semantic_neighbourhood_query(heading_entries, article_entries)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'header': {'type': 'esearch', 'version': '0.3'},\n",
       " 'esearchresult': {'count': '90797',\n",
       "  'retmax': '5',\n",
       "  'retstart': '0',\n",
       "  'idlist': ['39243071', '39242619', '39242503', '39242108', '39240939'],\n",
       "  'translationset': [],\n",
       "  'querytranslation': '(\"Biomarkers\"[MeSH Terms] AND \"Severity of Illness Index\"[MeSH Terms]) OR (\"amino acids, branched chain\"[MeSH Terms] AND \"Dietary Supplements\"[MeSH Terms] AND \"Muscle Strength\"[MeSH Terms]) OR \"Acute Kidney Injury\"[MeSH Terms] OR \"Nomograms\"[MeSH Terms]',\n",
       "  'warninglist': {'phrasesignored': [],\n",
       "   'quotedphrasesnotfound': [],\n",
       "   'outputmessages': ['AND', ')']}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper.pubmed_search as pubs\n",
    "pubs.get_query_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'querytranslation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m requery \u001b[38;5;241m=\u001b[39m openai_qe_manager\u001b[38;5;241m.\u001b[39mcreate_semantic_neighbourhood_query(heading_entries, article_entries)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mopenai_qe_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_retrieved_article_details_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m requeried_heading_entries, requeried_article_entries \u001b[38;5;241m=\u001b[39m openai_qe_manager\u001b[38;5;241m.\u001b[39mget_filtered_entries_from_retrieved_article_details(\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      4\u001b[0m requeried_article_entries_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(requeried_article_entries)\u001b[38;5;241m.\u001b[39mdrop_duplicates([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuitability\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\DataMicron\\Desktop\\Github\\Information-Retrieval\\helper\\pubmed_search.py:70\u001b[0m, in \u001b[0;36mQueryExpansionManager.get_retrieved_article_details_from_query\u001b[1;34m(self, query, requery, remove_stop_words)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m requery:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieved_article_details, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslated_query \u001b[38;5;241m=\u001b[39m \u001b[43mextract_article_details_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_stop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DataMicron\\Desktop\\Github\\Information-Retrieval\\helper\\pubmed_search.py:250\u001b[0m, in \u001b[0;36mextract_article_details_from_query\u001b[1;34m(query, articles_to_retrieve, remove_stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_article_details_from_query\u001b[39m(query, articles_to_retrieve\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, remove_stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Get response from entrez first via esearch\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     query_response \u001b[38;5;241m=\u001b[39m get_query_response(query, remove_stop_words\u001b[38;5;241m=\u001b[39mremove_stop_words, articles_to_retrieve\u001b[38;5;241m=\u001b[39marticles_to_retrieve)\n\u001b[1;32m--> 250\u001b[0m     translated_query \u001b[38;5;241m=\u001b[39m \u001b[43mquery_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mesearchresult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquerytranslation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    251\u001b[0m     retrieved_ids \u001b[38;5;241m=\u001b[39m query_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mesearchresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midlist\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# Extract all the details of esearch's retrieved articles using efetch\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'querytranslation'"
     ]
    }
   ],
   "source": [
    "requery = blade_embed_qe_manager.create_semantic_neighbourhood_query(heading_entries, article_entries)\n",
    "blade_embed_qe_manager.get_retrieved_article_details_from_query(requery, requery=True, remove_stop_words=False)\n",
    "requeried_heading_entries, requeried_article_entries = blade_embed_qe_manager.get_filtered_entries_from_retrieved_article_details(0.2, 0.2)\n",
    "requeried_article_entries_df = pd.DataFrame(requeried_article_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)\n",
    "requeried_article_entries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39242688',\n",
       " '38125829',\n",
       " '38018513',\n",
       " '39119581',\n",
       " '39242716',\n",
       " '38355242',\n",
       " '38643268',\n",
       " '39243024',\n",
       " '38402855',\n",
       " '38298766']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([article_entries_df, requeried_article_entries_df]).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False).head(10)['name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_entries, article_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_neighbourhoods = []\n",
    "for article_entry in article_entries:\n",
    "    expansion_set = article_entry['headings']\n",
    "    if len(expansion_set)!=0:\n",
    "        chunk_fragments_to_expand = []\n",
    "        for i, val in enumerate(expansion_set):\n",
    "            # First, vet out mesh terms deemed not relevant to the search\n",
    "            if val not in heading_entries:\n",
    "                continue\n",
    "            if i == 0: # first term of set\n",
    "                chunk_fragments_to_expand.append(f'(\"{val}\"[MeSH Terms]')\n",
    "            elif i == (len(expansion_set)-1): # final term of set\n",
    "                chunk_fragments_to_expand.append(f'AND \"{val}\"[MeSH Terms])')\n",
    "            else:\n",
    "                chunk_fragments_to_expand.append(f'AND \"{val}\"[MeSH Terms]')\n",
    "        article_semantic_neighbourhood = ' '.join(chunk_fragments_to_expand)\n",
    "        if article_semantic_neighbourhood!= '':\n",
    "            semantic_neighbourhoods.append(article_semantic_neighbourhood)\n",
    "semantic_neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if semantic_neighbourhoods != []:\n",
    "    combination_of_semantic_neighbourhoods = \" OR \".join(semantic_neighbourhoods)\n",
    "else: \n",
    "    combination_of_semantic_neighbourhoods = ''\n",
    "combination_of_semantic_neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])AND ((\"Asthma\"[MeSH Terms] AND \"COVID-19\"[MeSH Terms] AND \"Exanthema\"[MeSH Terms]) OR (\"Cough\"[MeSH Terms] AND \"Exanthema\"[MeSH Terms] AND \"Sneezing\"[MeSH Terms]))'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_query = translated_query + 'AND (' + combination_of_semantic_neighbourhoods + ')'\n",
    "expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'header': {'type': 'esearch', 'version': '0.3'},\n",
       " 'esearchresult': {'count': '4',\n",
       "  'retmax': '4',\n",
       "  'retstart': '0',\n",
       "  'idlist': ['35624503', '33627147', '29795198', '13020435'],\n",
       "  'translationset': [],\n",
       "  'querytranslation': '(\"Asthma\"[MeSH Terms] AND \"COVID-19\"[MeSH Terms] AND \"Exanthema\"[MeSH Terms]) OR (\"Cough\"[MeSH Terms] AND \"Exanthema\"[MeSH Terms] AND \"Sneezing\"[MeSH Terms])'}}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs.get_query_response(combination_of_semantic_neighbourhoods, preprocess=False, articles_to_retrieve=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'35624503': {'Title': 'Allergic shiners in a patient with cough-variant asthma: a case report.',\n",
       "  'Abstract': 'Chronic cough, with a duration of coughing of more than 8 weeks in adults, affects 5-10% of the general population. One of the most common causes of chronic cough is cough-variant asthma, which accounts for approximately one-third of cases. This phenotype of asthma is characterized by extreme sensitivity of the neuronal pathways mediating cough to environmental irritants, which results in an urge to cough. This case is an example of cough-variant asthma presenting with allergic shiners due to her severe cough.',\n",
       "  'Mesh Headings': [{'Descriptor': 'Asthma',\n",
       "    'Descriptor UI': 'D001249',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'COVID-19',\n",
       "    'Descriptor UI': 'D000086382',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Cyprinidae',\n",
       "    'Descriptor UI': 'D003530',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Exanthema',\n",
       "    'Descriptor UI': 'D005076',\n",
       "    'Is Major Topic': 'Y'}]},\n",
       " '33627147': {'Title': 'Distinguishing active pediatric COVID-19 pneumonia from MIS-C.',\n",
       "  'Abstract': 'Active pediatric COVID-19 pneumonia and MIS-C are two disease processes requiring rapid diagnosis and different treatment protocols.',\n",
       "  'Mesh Headings': []},\n",
       " '29795198': {'Title': 'The association of ambient PM',\n",
       "  'Abstract': 'Children are a susceptible population to exposure of ambient fine particulate air pollution (PM',\n",
       "  'Mesh Headings': [{'Descriptor': 'Absenteeism',\n",
       "    'Descriptor UI': 'D000041',\n",
       "    'Is Major Topic': 'Y'}]},\n",
       " '13020435': {'Title': '[Development of normal reflexes and reactions into abnormal, e.g. coughing, sneezing, nettle rash and purples].',\n",
       "  'Abstract': None,\n",
       "  'Mesh Headings': [{'Descriptor': 'Cough',\n",
       "    'Descriptor UI': 'D003371',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Exanthema',\n",
       "    'Descriptor UI': 'D005076',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Nervous System Physiological Phenomena',\n",
       "    'Descriptor UI': 'D009424',\n",
       "    'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Reflex', 'Descriptor UI': 'D012018', 'Is Major Topic': 'Y'},\n",
       "   {'Descriptor': 'Sneezing',\n",
       "    'Descriptor UI': 'D012912',\n",
       "    'Is Major Topic': 'Y'}]}}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_retrieved_article_details, new_translated_query = pubs.extract_article_details_from_query(combination_of_semantic_neighbourhoods, remove_stop_words=False, articles_to_retrieve=20)\n",
    "new_retrieved_article_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>headings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35624503</td>\n",
       "      <td>Allergic shiners in a patient with cough-varia...</td>\n",
       "      <td>[[-0.2013416, 0.4105001, -0.39661187, 0.395849...</td>\n",
       "      <td>[Asthma, COVID-19, Cyprinidae, Exanthema]</td>\n",
       "      <td>63.176656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33627147</td>\n",
       "      <td>Distinguishing active pediatric COVID-19 pneum...</td>\n",
       "      <td>[[-0.0075050797, 0.047646612, -0.38775554, 1.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>63.176656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13020435</td>\n",
       "      <td>[Development of normal reflexes and reactions ...</td>\n",
       "      <td>[[-0.6718215, -0.034178186, 0.235743, -0.10054...</td>\n",
       "      <td>[Cough, Exanthema, Nervous System Physiologica...</td>\n",
       "      <td>59.345675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29795198</td>\n",
       "      <td>The association of ambient PMChildren are a su...</td>\n",
       "      <td>[[-0.38462546, 0.7128517, 0.17911923, 0.227644...</td>\n",
       "      <td>[Absenteeism]</td>\n",
       "      <td>50.948042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                            content  \\\n",
       "0  35624503  Allergic shiners in a patient with cough-varia...   \n",
       "1  33627147  Distinguishing active pediatric COVID-19 pneum...   \n",
       "3  13020435  [Development of normal reflexes and reactions ...   \n",
       "2  29795198  The association of ambient PMChildren are a su...   \n",
       "\n",
       "                                          embeddings  \\\n",
       "0  [[-0.2013416, 0.4105001, -0.39661187, 0.395849...   \n",
       "1  [[-0.0075050797, 0.047646612, -0.38775554, 1.0...   \n",
       "3  [[-0.6718215, -0.034178186, 0.235743, -0.10054...   \n",
       "2  [[-0.38462546, 0.7128517, 0.17911923, 0.227644...   \n",
       "\n",
       "                                            headings  suitability  \n",
       "0          [Asthma, COVID-19, Cyprinidae, Exanthema]    63.176656  \n",
       "1                                                 []    63.176656  \n",
       "3  [Cough, Exanthema, Nervous System Physiologica...    59.345675  \n",
       "2                                      [Absenteeism]    50.948042  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_heading_entries, expanded_article_entries = blade_embed_qe_manager.extract_filtered_entries_from_retrieved_article_details(new_retrieved_article_details, combination_of_semantic_neighbourhoods, title_abstract_threshold, heading_threshold)\n",
    "pd.DataFrame(expanded_article_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract relevant MeSH keywords and add it to a list for requery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(\"purple\"[All Fields] OR \"purples\"[All Fields])',\n",
       " 'AND',\n",
       " '(\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields])',\n",
       " 'AND',\n",
       " '(\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_query_fragments_and_connectors(s):\n",
    "    result = []\n",
    "    temp = ''\n",
    "    inside_parentheses = False    \n",
    "    for char in s:\n",
    "        if char == '(':\n",
    "            if temp:\n",
    "                result.append(temp.strip())  # Add content before '(' to the list\n",
    "                temp = ''\n",
    "            inside_parentheses = True\n",
    "            temp += char  # Add '(' to the temp\n",
    "        elif char == ')':\n",
    "            temp += char  # Add ')' to the temp\n",
    "            result.append(temp.strip())  # Add content inside parentheses to the list\n",
    "            temp = ''\n",
    "            inside_parentheses = False\n",
    "        else:\n",
    "            temp += char    \n",
    "    if temp:  # If there is any remaining content, add it to the list\n",
    "        result.append(temp.strip())    \n",
    "    return result\n",
    "\n",
    "print(translated_query)\n",
    "extracted_query_fragments = extract_query_fragments_and_connectors(translated_query)\n",
    "extracted_query_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diabetes', 'Hypertension', 'Cancer', None, 'Heart Disease', 'exanthema']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Example strings\n",
    "strings = [\n",
    "    '\"Diabetes\"[MeSH Terms]',\n",
    "    '\"Hypertension\"[MeSH Terms]',\n",
    "    '\"Cancer\"[MeSH Terms]',\n",
    "    'Random text',\n",
    "    '\"Heart Disease\"[MeSH Terms] is a condition.',\n",
    "    '(\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields])'\n",
    "]\n",
    "pattern = r'\"(.*?)\"\\[MeSH Terms\\]'\n",
    "\n",
    "[re.search(pattern, s).group(1) if re.search(pattern, s) else None for s in strings]\n",
    "\n",
    "extracted_terms = [re.search(pattern, s).group(1) if re.search(pattern, s) else None for s in strings]\n",
    "print(extracted_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields])'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'(\"purple\"[All Fields] OR \"purples\"[All Fields])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"purple\"[All Fields] OR \"purples\"[All Fields])\n",
      "<class 'str'>\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "AND\n",
      "<class 'str'>\n",
      "[None, None, None]\n",
      "(\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields])\n",
      "<class 'str'>\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "AND\n",
      "<class 'str'>\n",
      "[None, None, None]\n",
      "(\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])\n",
      "<class 'str'>\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "def extract_terms_from_translated_set(extracted_query_fragments):\n",
    "    pattern = r'\"(.*?)\"\\[MeSH Terms\\]'\n",
    "    atm_terms = {}\n",
    "    for fragment in extracted_query_fragments:\n",
    "        print(fragment)\n",
    "        print(type(fragment))\n",
    "        extracted_terms = [re.search(pattern, s).group(1) if re.search(pattern, s) else None for s in fragment]\n",
    "        print(extracted_terms)\n",
    "    #     if extracted_terms is not None:\n",
    "    #         atm_terms[extracted_terms] = fragment\n",
    "    # return atm_terms\n",
    "\n",
    "extract_terms_from_translated_set(extracted_query_fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\"cough\"[MeSH Terms]' in extracted_query_fragments[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Asthma',\n",
       " 'COVID-19',\n",
       " 'Cough',\n",
       " 'Cyprinidae',\n",
       " 'Exanthema',\n",
       " 'Nervous System Physiological Phenomena',\n",
       " 'Reflex',\n",
       " 'Sneezing'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expansion_set = set()\n",
    "for article_entry in article_entries:\n",
    "    # print(filtered_article['name'])\n",
    "    for individual_heading_entry in retrieved_article_details[article_entry['name']]['Mesh Headings']:\n",
    "        expansion_set.add(individual_heading_entry['Descriptor'])\n",
    "    expansion_set\n",
    "expansion_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Synonyms': ['Exanthema', 'Exanthem', 'Rash', 'Skin Rash', 'Rash, Skin'],\n",
       " 'Annotation': 'a skin eruption usually chem- or viral-induced; EXANTHEMA SUBITUM is also available\\n  ',\n",
       " 'Scope Note': \"Diseases in which skin eruptions or rashes are a prominent manifestation. Classically, six such diseases were described with similar rashes; they were numbered in the order in which they were reported. Only the fourth (Duke's disease), fifth (ERYTHEMA INFECTIOSUM), and sixth (EXANTHEMA SUBITUM) numeric designations survive as occasional synonyms in current terminology.\\n    \",\n",
       " 'Combined Description': \"Diseases in which skin eruptions or rashes are a prominent manifestation. Classically, six such diseases were described with similar rashes; they were numbered in the order in which they were reported. Only the fourth (Duke's disease), fifth (ERYTHEMA INFECTIOSUM), and sixth (EXANTHEMA SUBITUM) numeric designations survive as occasional synonyms in current terminology.\\n    a skin eruption usually chem- or viral-induced; EXANTHEMA SUBITUM is also available\\n  \"}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors['Exanthema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"Cough\"[MeSH Terms] OR \"COVID-19\"[MeSH Terms] OR \"Cyprinidae\"[MeSH Terms] OR \"Reflex\"[MeSH Terms] OR \"Asthma\"[MeSH Terms] OR \"Exanthema\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms] OR \"Sneezing\"[MeSH Terms])'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expanded_query_fragments = extracted_query_fragments\n",
    "chunk_fragments_to_expand = []\n",
    "for i, val in enumerate(expansion_set):\n",
    "    if i == 0: # first term of set\n",
    "        chunk_fragments_to_expand.append(f'(\"{val}\"[MeSH Terms]')\n",
    "    elif i == (len(expansion_set)-1): # final term of set\n",
    "        chunk_fragments_to_expand.append(f'OR \"{val}\"[MeSH Terms])')\n",
    "    else:\n",
    "        chunk_fragments_to_expand.append(f'OR \"{val}\"[MeSH Terms]')\n",
    "chunk_to_expand = ' '.join(chunk_fragments_to_expand)\n",
    "chunk_to_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"Exanthema\"[MeSH Terms] OR \"Exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"Cough\"[MeSH Terms] OR \"Cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields]) AND (\"Cough\"[MeSH Terms] OR \"COVID-19\"[MeSH Terms] OR \"Cyprinidae\"[MeSH Terms] OR \"Reflex\"[MeSH Terms] OR \"Asthma\"[MeSH Terms] OR \"Exanthema\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms] OR \"Sneezing\"[MeSH Terms])'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_expanded_article_details, translated_expanded_query = pubs.extract_article_details_from_query(translated_query + ' AND ' + chunk_to_expand, remove_stop_words=False, articles_to_retrieve=20)\n",
    "translated_expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Cough\"[MeSH Terms] OR \"COVID-19\"[MeSH Terms] OR \"Cyprinidae\"[MeSH Terms] OR \"Reflex\"[MeSH Terms] OR \"Asthma\"[MeSH Terms] OR \"Exanthema\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms] OR \"Sneezing\"[MeSH Terms]'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"Exanthema\"[MeSH Terms] OR \"Exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"Cough\"[MeSH Terms] OR \"Cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields]) AND (\"COVID-19\"[MeSH Terms] OR \"Cyprinidae\"[MeSH Terms] OR \"Reflex\"[MeSH Terms] OR \"Asthma\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms] OR \"Sneezing\"[MeSH Terms])'\n",
    "retrieved_expanded_article_details, translated_expanded_query = pubs.extract_article_details_from_query(chunk_to_expand, remove_stop_words=False, articles_to_retrieve=20)\n",
    "translated_expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>A viral disorder generally characterized by hi...</td>\n",
       "      <td>[[-0.010197605, -0.21895505, 0.11601348, -0.09...</td>\n",
       "      <td>43.666178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Immunization, Secondary</td>\n",
       "      <td>Any immunization following a primary immunizat...</td>\n",
       "      <td>[[0.033557262, -0.31072006, 0.18211477, -0.170...</td>\n",
       "      <td>33.800560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Vaccination</td>\n",
       "      <td>Administration of vaccines to stimulate the ho...</td>\n",
       "      <td>[[0.024296435, -0.23892102, 0.09250141, -0.150...</td>\n",
       "      <td>30.236161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Disease Models, Animal</td>\n",
       "      <td>Naturally-occurring or experimentally-induced ...</td>\n",
       "      <td>[[0.021185394, -0.026043989, 0.1372653, -0.159...</td>\n",
       "      <td>29.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COVID-19 Drug Treatment</td>\n",
       "      <td>The use of DRUGS to treat COVID19 or its sympt...</td>\n",
       "      <td>[[-0.05611608, -0.05003875, 0.037707295, -0.10...</td>\n",
       "      <td>27.319974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARS-CoV-2</td>\n",
       "      <td>A species of BETACORONAVIRUS causing atypical ...</td>\n",
       "      <td>[[0.053536292, -0.10688641, 0.060441017, -0.12...</td>\n",
       "      <td>24.721570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Antibodies, Viral</td>\n",
       "      <td>Immunoglobulins produced in response to VIRAL ...</td>\n",
       "      <td>[[0.023101866, -0.1811388, 0.2032994, -0.01521...</td>\n",
       "      <td>24.048373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COVID-19 Vaccines</td>\n",
       "      <td>Vaccines or candidate vaccines containing SARS...</td>\n",
       "      <td>[[0.04259034, -0.112093695, 0.054803435, -0.06...</td>\n",
       "      <td>23.301151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Asthma</td>\n",
       "      <td>A form of bronchial disorder with three distin...</td>\n",
       "      <td>[[-0.100331, -0.28149906, -0.009670261, -0.090...</td>\n",
       "      <td>21.999471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Vaccines, Subunit</td>\n",
       "      <td>Vaccines consisting of one or more antigens th...</td>\n",
       "      <td>[[0.028799502, -0.0485275, 0.094096035, -0.109...</td>\n",
       "      <td>21.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>T-Lymphocytes, Cytotoxic</td>\n",
       "      <td>Immunized T-lymphocytes which can directly des...</td>\n",
       "      <td>[[-0.007253751, -0.2498892, 0.0932693, 0.03282...</td>\n",
       "      <td>21.596168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>BNT162 Vaccine</td>\n",
       "      <td>mRNA vaccine against SARS-CoV-2 developed by P...</td>\n",
       "      <td>[[0.05458549, 0.035802208, 0.05859151, -0.0120...</td>\n",
       "      <td>20.413753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Demyelinating Diseases</td>\n",
       "      <td>Diseases characterized by loss or dysfunction ...</td>\n",
       "      <td>[[-0.09033705, -0.1731948, 0.0977308, 0.063826...</td>\n",
       "      <td>18.406633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Spike Glycoprotein, Coronavirus</td>\n",
       "      <td>A class I viral fusion protein that forms the ...</td>\n",
       "      <td>[[0.020307178, -0.36207038, 0.10541501, 0.0238...</td>\n",
       "      <td>17.490754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Antiviral Agents</td>\n",
       "      <td>Agents used in the prophylaxis or therapy of V...</td>\n",
       "      <td>[[0.07561414, -0.23896714, -0.06138481, -0.029...</td>\n",
       "      <td>17.261556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Viral Load</td>\n",
       "      <td>The quantity of measurable virus in a body flu...</td>\n",
       "      <td>[[-0.019718254, -0.34094143, 0.0039595254, -0....</td>\n",
       "      <td>15.751019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virus Replication</td>\n",
       "      <td>The process of intracellular viral multiplicat...</td>\n",
       "      <td>[[0.012798517, -0.3610555, 0.17097086, -0.1070...</td>\n",
       "      <td>15.013064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Osteoarthritis, Knee</td>\n",
       "      <td>Noninflammatory degenerative disease of the kn...</td>\n",
       "      <td>[[-0.15851419, -0.09339724, -0.07821148, 0.063...</td>\n",
       "      <td>14.307684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Remyelination</td>\n",
       "      <td>The reforming of the MYELIN SHEATH around AXON...</td>\n",
       "      <td>[[-0.15355273, -0.010209496, 0.1419328, 0.0265...</td>\n",
       "      <td>13.243926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B-Lymphocytes</td>\n",
       "      <td>Lymphoid cells concerned with humoral immunity...</td>\n",
       "      <td>[[-0.030508801, -0.10058115, 0.026581839, -0.0...</td>\n",
       "      <td>11.839297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Seizures</td>\n",
       "      <td>Clinical or subclinical disturbances of cortic...</td>\n",
       "      <td>[[-0.04942192, -0.15429002, 0.0780064, -0.0795...</td>\n",
       "      <td>11.489199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Clusterin</td>\n",
       "      <td>A highly conserved heterodimeric glycoprotein ...</td>\n",
       "      <td>[[-0.038416047, -0.18574968, 0.060524184, -0.0...</td>\n",
       "      <td>10.321529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "2                          COVID-19   \n",
       "32          Immunization, Secondary   \n",
       "35                      Vaccination   \n",
       "9            Disease Models, Animal   \n",
       "14          COVID-19 Drug Treatment   \n",
       "1                        SARS-CoV-2   \n",
       "30                Antibodies, Viral   \n",
       "6                 COVID-19 Vaccines   \n",
       "28                           Asthma   \n",
       "24                Vaccines, Subunit   \n",
       "26         T-Lymphocytes, Cytotoxic   \n",
       "34                   BNT162 Vaccine   \n",
       "20           Demyelinating Diseases   \n",
       "21  Spike Glycoprotein, Coronavirus   \n",
       "16                 Antiviral Agents   \n",
       "12                       Viral Load   \n",
       "0                 Virus Replication   \n",
       "8              Osteoarthritis, Knee   \n",
       "18                    Remyelination   \n",
       "25                    B-Lymphocytes   \n",
       "10                         Seizures   \n",
       "17                        Clusterin   \n",
       "\n",
       "                                              content  \\\n",
       "2   A viral disorder generally characterized by hi...   \n",
       "32  Any immunization following a primary immunizat...   \n",
       "35  Administration of vaccines to stimulate the ho...   \n",
       "9   Naturally-occurring or experimentally-induced ...   \n",
       "14  The use of DRUGS to treat COVID19 or its sympt...   \n",
       "1   A species of BETACORONAVIRUS causing atypical ...   \n",
       "30  Immunoglobulins produced in response to VIRAL ...   \n",
       "6   Vaccines or candidate vaccines containing SARS...   \n",
       "28  A form of bronchial disorder with three distin...   \n",
       "24  Vaccines consisting of one or more antigens th...   \n",
       "26  Immunized T-lymphocytes which can directly des...   \n",
       "34  mRNA vaccine against SARS-CoV-2 developed by P...   \n",
       "20  Diseases characterized by loss or dysfunction ...   \n",
       "21  A class I viral fusion protein that forms the ...   \n",
       "16  Agents used in the prophylaxis or therapy of V...   \n",
       "12  The quantity of measurable virus in a body flu...   \n",
       "0   The process of intracellular viral multiplicat...   \n",
       "8   Noninflammatory degenerative disease of the kn...   \n",
       "18  The reforming of the MYELIN SHEATH around AXON...   \n",
       "25  Lymphoid cells concerned with humoral immunity...   \n",
       "10  Clinical or subclinical disturbances of cortic...   \n",
       "17  A highly conserved heterodimeric glycoprotein ...   \n",
       "\n",
       "                                           embeddings  suitability  \n",
       "2   [[-0.010197605, -0.21895505, 0.11601348, -0.09...    43.666178  \n",
       "32  [[0.033557262, -0.31072006, 0.18211477, -0.170...    33.800560  \n",
       "35  [[0.024296435, -0.23892102, 0.09250141, -0.150...    30.236161  \n",
       "9   [[0.021185394, -0.026043989, 0.1372653, -0.159...    29.191500  \n",
       "14  [[-0.05611608, -0.05003875, 0.037707295, -0.10...    27.319974  \n",
       "1   [[0.053536292, -0.10688641, 0.060441017, -0.12...    24.721570  \n",
       "30  [[0.023101866, -0.1811388, 0.2032994, -0.01521...    24.048373  \n",
       "6   [[0.04259034, -0.112093695, 0.054803435, -0.06...    23.301151  \n",
       "28  [[-0.100331, -0.28149906, -0.009670261, -0.090...    21.999471  \n",
       "24  [[0.028799502, -0.0485275, 0.094096035, -0.109...    21.727273  \n",
       "26  [[-0.007253751, -0.2498892, 0.0932693, 0.03282...    21.596168  \n",
       "34  [[0.05458549, 0.035802208, 0.05859151, -0.0120...    20.413753  \n",
       "20  [[-0.09033705, -0.1731948, 0.0977308, 0.063826...    18.406633  \n",
       "21  [[0.020307178, -0.36207038, 0.10541501, 0.0238...    17.490754  \n",
       "16  [[0.07561414, -0.23896714, -0.06138481, -0.029...    17.261556  \n",
       "12  [[-0.019718254, -0.34094143, 0.0039595254, -0....    15.751019  \n",
       "0   [[0.012798517, -0.3610555, 0.17097086, -0.1070...    15.013064  \n",
       "8   [[-0.15851419, -0.09339724, -0.07821148, 0.063...    14.307684  \n",
       "18  [[-0.15355273, -0.010209496, 0.1419328, 0.0265...    13.243926  \n",
       "25  [[-0.030508801, -0.10058115, 0.026581839, -0.0...    11.839297  \n",
       "10  [[-0.04942192, -0.15429002, 0.0780064, -0.0795...    11.489199  \n",
       "17  [[-0.038416047, -0.18574968, 0.060524184, -0.0...    10.321529  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_heading_entries, expanded_article_entries = qe_manager.extract_filtered_entries_from_retrieved_article_details(retrieved_expanded_article_details, query, title_abstract_threshold, heading_threshold)\n",
    "pd.DataFrame(expanded_heading_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39242658</td>\n",
       "      <td>SARS-CoV-2 viral load is linked to remdesivir ...</td>\n",
       "      <td>[[-0.010255081, -0.09798241, -0.028058521, -0....</td>\n",
       "      <td>17.261556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                            content  \\\n",
       "0  39242658  SARS-CoV-2 viral load is linked to remdesivir ...   \n",
       "\n",
       "                                          embeddings  suitability  \n",
       "0  [[-0.010255081, -0.09798241, -0.028058521, -0....    17.261556  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(expanded_article_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields]) OR \"Cough\"[MeSH Terms] OR \"cough\"[MeSH Terms]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_query_fragments.append(\"OR\")\n",
    "extracted_query_fragments.append('\"cough\"[MeSH Terms]')\n",
    "test_statement = \" \".join(extracted_query_fragments)\n",
    "test_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Cough\"[MeSH Terms]\n",
    "\"COVID-19\"[MeSH Terms]\n",
    "\"Cyprinidae\"[MeSH Terms]\n",
    "\"Reflex\"[MeSH Terms]\n",
    "\"Nervous System Physiological Phenomena\"[MeSH Terms]\n",
    "\"Exanthema\"[MeSH Terms]\n",
    "\"Asthma\"[MeSH Terms]\n",
    "\"Sneezing\"[MeSH Terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exanthema</td>\n",
       "      <td>Diseases in which skin eruptions or rashes are...</td>\n",
       "      <td>[[0.09581368, -0.26140502, 0.117319815, 0.0547...</td>\n",
       "      <td>43.781447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>A viral disorder generally characterized by hi...</td>\n",
       "      <td>[[-0.010197605, -0.21895505, 0.11601348, -0.09...</td>\n",
       "      <td>43.666178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asthma</td>\n",
       "      <td>A form of bronchial disorder with three distin...</td>\n",
       "      <td>[[-0.100331, -0.28149906, -0.009670261, -0.090...</td>\n",
       "      <td>21.999471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough</td>\n",
       "      <td>A sudden, audible expulsion of air from the lu...</td>\n",
       "      <td>[[-0.11444584, -0.4871452, 0.07324287, -0.0474...</td>\n",
       "      <td>19.485989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sneezing</td>\n",
       "      <td>The sudden, forceful, involuntary expulsion of...</td>\n",
       "      <td>[[-0.14127521, -0.43756396, 0.11732302, -0.123...</td>\n",
       "      <td>19.093876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                            content  \\\n",
       "2  Exanthema  Diseases in which skin eruptions or rashes are...   \n",
       "1   COVID-19  A viral disorder generally characterized by hi...   \n",
       "0     Asthma  A form of bronchial disorder with three distin...   \n",
       "3      Cough  A sudden, audible expulsion of air from the lu...   \n",
       "5   Sneezing  The sudden, forceful, involuntary expulsion of...   \n",
       "\n",
       "                                          embeddings  suitability  \n",
       "2  [[0.09581368, -0.26140502, 0.117319815, 0.0547...    43.781447  \n",
       "1  [[-0.010197605, -0.21895505, 0.11601348, -0.09...    43.666178  \n",
       "0  [[-0.100331, -0.28149906, -0.009670261, -0.090...    21.999471  \n",
       "3  [[-0.11444584, -0.4871452, 0.07324287, -0.0474...    19.485989  \n",
       "5  [[-0.14127521, -0.43756396, 0.11732302, -0.123...    19.093876  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_statement = '(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields]) AND (\"covid-19\"[MeSH Terms] OR \"cyprinidae\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms])'\n",
    "test_article_details, translated_query = pubs.extract_article_details_from_query(test_statement, remove_stop_words=False, articles_to_retrieve=10)\n",
    "test_heading_entries, test_article_entries = qe_manager.extract_filtered_entries_from_retrieved_article_details(test_article_details, query, title_abstract_threshold, heading_threshold)\n",
    "pd.DataFrame(test_heading_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"cough\"[MeSH Terms] OR \"cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields]) AND (\"covid-19\"[MeSH Terms] OR \"cyprinidae\"[MeSH Terms] OR \"Nervous System Physiological Phenomena\"[MeSH Terms])'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35624503</td>\n",
       "      <td>Allergic shiners in a patient with cough-varia...</td>\n",
       "      <td>[[0.010772239, -0.1761123, 0.07280753, 0.05663...</td>\n",
       "      <td>43.781447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13020435</td>\n",
       "      <td>[Development of normal reflexes and reactions ...</td>\n",
       "      <td>[[-0.049132317, -0.15629517, 0.13611218, -0.01...</td>\n",
       "      <td>19.093876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                            content  \\\n",
       "0  35624503  Allergic shiners in a patient with cough-varia...   \n",
       "1  13020435  [Development of normal reflexes and reactions ...   \n",
       "\n",
       "                                          embeddings  suitability  \n",
       "0  [[0.010772239, -0.1761123, 0.07280753, 0.05663...    43.781447  \n",
       "1  [[-0.049132317, -0.15629517, 0.13611218, -0.01...    19.093876  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_article_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39217183</td>\n",
       "      <td>Mycobacterium tuberculosis cough aerosol cultu...</td>\n",
       "      <td>[[0.046328314, 0.047822025, -0.049906746, 0.00...</td>\n",
       "      <td>37.748751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39222970</td>\n",
       "      <td>Chronic cough in preschool aged children.</td>\n",
       "      <td>[[0.016885558, 0.18008955, 0.058235824, -0.001...</td>\n",
       "      <td>19.485989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39219607</td>\n",
       "      <td>Cough-induced severe bradycardia and syncope i...</td>\n",
       "      <td>[[-0.0027521388, -0.032782163, 0.059245188, -0...</td>\n",
       "      <td>5.422820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39235438</td>\n",
       "      <td>Content validity of the Leicester Cough Questi...</td>\n",
       "      <td>[[0.03931393, 0.15439567, 0.018702619, -0.0283...</td>\n",
       "      <td>-8.319442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                            content  \\\n",
       "3  39217183  Mycobacterium tuberculosis cough aerosol cultu...   \n",
       "1  39222970          Chronic cough in preschool aged children.   \n",
       "2  39219607  Cough-induced severe bradycardia and syncope i...   \n",
       "0  39235438  Content validity of the Leicester Cough Questi...   \n",
       "\n",
       "                                          embeddings  suitability  \n",
       "3  [[0.046328314, 0.047822025, -0.049906746, 0.00...    37.748751  \n",
       "1  [[0.016885558, 0.18008955, 0.058235824, -0.001...    19.485989  \n",
       "2  [[-0.0027521388, -0.032782163, 0.059245188, -0...     5.422820  \n",
       "0  [[0.03931393, 0.15439567, 0.018702619, -0.0283...    -8.319442  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_article_entries).drop_duplicates(['name']).sort_values(by=['suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>Suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28759392</td>\n",
       "      <td>A 3-Week-Old With an Isolated \"Blueberry Muffi...</td>\n",
       "      <td>[[0.030234266, -0.13129903, 0.12427848, -0.070...</td>\n",
       "      <td>47.270375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13020435</td>\n",
       "      <td>[Development of normal reflexes and reactions ...</td>\n",
       "      <td>[[-0.049132317, -0.15629517, 0.13611218, -0.01...</td>\n",
       "      <td>46.364245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35624503</td>\n",
       "      <td>Allergic shiners in a patient with cough-varia...</td>\n",
       "      <td>[[0.010772239, -0.1761123, 0.07280753, 0.05663...</td>\n",
       "      <td>42.842036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                            content  \\\n",
       "1  28759392  A 3-Week-Old With an Isolated \"Blueberry Muffi...   \n",
       "2  13020435  [Development of normal reflexes and reactions ...   \n",
       "0  35624503  Allergic shiners in a patient with cough-varia...   \n",
       "\n",
       "                                          embeddings  Suitability  \n",
       "1  [[0.030234266, -0.13129903, 0.12427848, -0.070...    47.270375  \n",
       "2  [[-0.049132317, -0.15629517, 0.13611218, -0.01...    46.364245  \n",
       "0  [[0.010772239, -0.1761123, 0.07280753, 0.05663...    42.842036  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(filtered_article_entries).drop_duplicates(['name']).sort_values(by=['Suitability'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requery stage....not sure how many articles are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'39238401': {'Title': '[Expert recommendations on the clinical application of mechanical insufflation-exsufflation in mechanically ventilated patients (2024)].',\n",
       "   'Abstract': 'Mechanical ventilated patients are a high-risk group with impaired cough ability and require corresponding medical techniques for cough assistance to clear airway secretions. Mechanical insufflation-exsufflation (MI-E) technology is widely used in patients with cough weakness caused by neuromuscular diseases. However, there is currently a lack of standardized application procedures for mechanically ventilated patients who retain artificial airways, which can affect treatment outcomes. Chinese Society of Critical Care Medicine organized experts including critical care physicians, nurses, respiratory therapists that focused on the clinical application of mechanical insufflation-exsufflation in mechanically ventilated patients. Through systematic collection, extraction, and summary of evidence-based clinical practice evidence and clinical experience, suggestions are proposed. Expert recommendations on the clinical application of mechanical insufflation-exsufflation in mechanically ventilated patients (2024) was formed by using improved Delphi method, hoping to provide references for standardized application of this technology. At the same time, those recommendations will also provide a reference for future clinical research on the application of mechanical insufflation-exsufflation technology in mechanically ventilated patients.',\n",
       "   'Mesh Headings': [{'Descriptor': 'Respiration, Artificial',\n",
       "     'Descriptor UI': 'D012121',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Insufflation',\n",
       "     'Descriptor UI': 'D007327',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39235438': {'Title': 'Content validity of the Leicester Cough Questionnaire in adults with refractory or unexplained chronic cough: a qualitative interview study.',\n",
       "   'Abstract': 'Chronic cough, a cough lasting >8\\u2009weeks, includes refractory chronic cough (RCC) and unexplained chronic cough (UCC). Patient-reported outcome (PRO) measures are needed to better understand chronic cough impacts that matter most to patients. The 19-item Leicester Cough Questionnaire (LCQ), an existing PRO measure of chronic cough, assesses impacts of cough across physical, psychological, and social domains. However, the content validity of the LCQ evaluating these concepts in patients with RCC/UCC had not been established.',\n",
       "   'Mesh Headings': [{'Descriptor': 'Cough',\n",
       "     'Descriptor UI': 'D003371',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Patient Reported Outcome Measures',\n",
       "     'Descriptor UI': 'D000071066',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Interviews as Topic',\n",
       "     'Descriptor UI': 'D007407',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39231560': {'Title': 'Treatment of left tenth rib haemangioma vascularised by a costal artery giving the artery of Adamkiewicz.',\n",
       "   'Abstract': 'Haemangioma of the ribs is considered an extremely rare benign tumour. Here, we present a case of a young male with left tenth rib haemangioma vascularised by a costal artery giving the artery of Adamkiewicz presented as chronic cough. This was successfully treated through preoperative embolisation and surgical resection. A preoperative angiogram was performed to identify the origin of the artery of Adamkiewicz. The final diagnosis was confirmed histopathologically. There were no complications in the postoperative course and no recurrence during 12 months of follow-up.',\n",
       "   'Mesh Headings': [{'Descriptor': 'Ribs',\n",
       "     'Descriptor UI': 'D012272',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Hemangioma',\n",
       "     'Descriptor UI': 'D006391',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39222970': {'Title': 'Chronic cough in preschool aged children.',\n",
       "   'Abstract': None,\n",
       "   'Mesh Headings': [{'Descriptor': 'Cough',\n",
       "     'Descriptor UI': 'D003371',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39222070': {'Title': 'Diagnosed and undiagnosed cough-related stress urinary incontinence in women with refractory or unexplained chronic cough: Its impact on general health status and quality of life.',\n",
       "   'Abstract': '',\n",
       "   'Mesh Headings': [{'Descriptor': 'Cough',\n",
       "     'Descriptor UI': 'D003371',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Quality of Life',\n",
       "     'Descriptor UI': 'D011788',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Urinary Incontinence, Stress',\n",
       "     'Descriptor UI': 'D014550',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Health Status',\n",
       "     'Descriptor UI': 'D006304',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39219607': {'Title': 'Cough-induced severe bradycardia and syncope in a dog.',\n",
       "   'Abstract': \"A 10-year-old spayed female shih tzu dog was brought to the hospital because of recurring syncope that occurred simultaneously with a cough. Physical examination did not reveal an abnormal heart rhythm or abnormal heart sounds. Electrocardiography revealed sinus arrest of 4.7 s with intermittent escape beats during coughing. Additional examinations, including thoracic radiography, clinical pathology, and echocardiography, revealed no abnormalities of concern. Forty-eight-hour Holter monitoring captured 1 syncopal episode following severe coughing, during which the longest sinus arrest lasted 16 s with intermittent escape beats. This observation confirmed our strong suspicion that coughing was the cause of varying degrees of sinus arrest in this dog. Theophylline, codeine, and short-term prednisolone were prescribed to treat the dog's cough. The daily episodes of syncope ceased and coughing decreased. Subsequent 48-hour Holter monitoring revealed no abnormal pauses, and the owner did not report syncope. Theophylline and codeine were continued for 5 mo, during which time no syncope occurred. To our knowledge, this case provides the first clear evidence of a correlation between cough-induced sinus arrest and syncope in a veterinary patient, as confirmed by Holter monitoring and electrocardiography. Key clinical message: Cough-induced severe bradycardia and syncope were identified in a shih tzu dog. After the antitussive medication was adjusted, the signs resolved.\",\n",
       "   'Mesh Headings': [{'Descriptor': 'Dog Diseases',\n",
       "     'Descriptor UI': 'D004283',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Syncope',\n",
       "     'Descriptor UI': 'D013575',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Cough', 'Descriptor UI': 'D003371', 'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Bradycardia',\n",
       "     'Descriptor UI': 'D001919',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39217183': {'Title': 'Mycobacterium tuberculosis cough aerosol culture status associates with host characteristics and inflammatory profiles.',\n",
       "   'Abstract': 'Interrupting transmission events is critical to tuberculosis control. Cough-generated aerosol cultures predict tuberculosis transmission better than microbiological or clinical markers. We hypothesize that highly infectious individuals with pulmonary tuberculosis (positive for cough aerosol cultures) have elevated inflammatory markers and unique transcriptional profiles compared to less infectious individuals. We performed a prospective, longitudinal study using cough aerosol sampling system. We enrolled 142 participants with treatment-nave pulmonary tuberculosis in Kenya and assessed the association of clinical, microbiologic, and immunologic characteristics with Mycobacterium tuberculosis aerosolization and transmission in 129 household members. Contacts of the forty-three aerosol culture-positive participants (30%) are more likely to have a positive interferon-gamma release assay (85% vs 53%, P\\u2009=\\u20090.006) and higher median IFN level (P\\u2009<\\u20090.001, 4.28 IU/ml (1.77-5.91) vs. 0.71 (0.01-3.56)) compared to aerosol culture-negative individuals. We find that higher bacillary burden, younger age, larger mean upper arm circumference, and host inflammatory profiles, including elevated serum C-reactive protein and lower plasma TNF levels, associate with positive cough aerosol cultures. Notably, we find pre-treatment whole blood transcriptional profiles associate with aerosol culture status, independent of bacillary load. These findings suggest that tuberculosis infectiousness is associated with epidemiologic characteristics and inflammatory signatures and that these features may identify highly infectious persons.',\n",
       "   'Mesh Headings': [{'Descriptor': 'Mycobacterium tuberculosis',\n",
       "     'Descriptor UI': 'D009169',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Cough', 'Descriptor UI': 'D003371', 'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Aerosols',\n",
       "     'Descriptor UI': 'D000336',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Tuberculosis, Pulmonary',\n",
       "     'Descriptor UI': 'D014397',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39212619': {'Title': 'The Respiratory Effects of Chronic Exposure to Gas Faring Among Residents of Some Communities in the Niger Delta Region of Nigeria.',\n",
       "   'Abstract': 'This study presents the pattern of respiratory effects seen among residents chronically exposed to gas flaring in some communities in the Niger Delta Region, Nigeria. The other health challenges associated with this chronic exposure to gas were also evaluated in the study.',\n",
       "   'Mesh Headings': [{'Descriptor': 'Environmental Exposure',\n",
       "     'Descriptor UI': 'D004781',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39210302': {'Title': 'Determinants of cough-related quality of life in interstitial lung diseases.',\n",
       "   'Abstract': 'Interstitial lung diseases (ILD) include a wide range of diseases impacting lung parenchyma and leading to fibrosis and architectural distortion. Chronic cough and dyspnea are common symptoms which affect the quality of life (QoL) in ILD patients. The mechanisms of cough in ILD patients are still unknown. The aim of this study was to prospectively investigate histological, radiological, and physiological determinants of cough-related QoL in ILD patients who underwent transbronchial lung cryobiopsy (TBLC).',\n",
       "   'Mesh Headings': [{'Descriptor': 'Lung Diseases, Interstitial',\n",
       "     'Descriptor UI': 'D017563',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Quality of Life',\n",
       "     'Descriptor UI': 'D011788',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Cough', 'Descriptor UI': 'D003371', 'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Tomography, X-Ray Computed',\n",
       "     'Descriptor UI': 'D014057',\n",
       "     'Is Major Topic': 'Y'}]},\n",
       "  '39202641': {'Title': 'The Impact of Long COVID on the Quality of Life.',\n",
       "   'Abstract': '',\n",
       "   'Mesh Headings': [{'Descriptor': 'COVID-19',\n",
       "     'Descriptor UI': 'D000086382',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Quality of Life',\n",
       "     'Descriptor UI': 'D011788',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'Post-Acute COVID-19 Syndrome',\n",
       "     'Descriptor UI': 'D000094024',\n",
       "     'Is Major Topic': 'Y'},\n",
       "    {'Descriptor': 'SARS-CoV-2',\n",
       "     'Descriptor UI': 'D000086402',\n",
       "     'Is Major Topic': 'Y'}]}},\n",
       " '((\"purple\"[All Fields] OR \"purples\"[All Fields]) AND (\"exanthema\"[MeSH Terms] OR \"exanthema\"[All Fields] OR \"rashes\"[All Fields]) AND (\"Cough\"[MeSH Terms] OR \"Cough\"[All Fields] OR \"coughing\"[All Fields] OR \"coughs\"[All Fields] OR \"coughed\"[All Fields])) OR \"Cough\"[MeSH Terms]')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article_details = pubs.extract_article_details_from_query(test_statement, remove_stop_words=False, articles_to_retrieve=10)\n",
    "heading_entries_test, article_entries_test = extract_content_and_embeddings_from_retrieved_article_details(test_article_details)\n",
    "filtered_heading_entries_test, filtered_article_entries_test = extract_filtered_entries_from_content_and_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Extracting Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('helper/descriptors.json') as json_file:\n",
    "    descriptors = json.load(json_file)\n",
    "with open('helper/concepts.json') as json_file:\n",
    "    concepts = json.load(json_file)\n",
    "with open('helper/terms.json') as json_file:\n",
    "    terms = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "1. Each word is broken down into semantic chunks\n",
    "2. For each semantic chunk, extract descriptor, term and preferred term (put them in an or chunk)\n",
    "3. Link all chunks together via AND\n",
    "4. If no articles returned, then have an option to remove words one by one\n",
    "5. (GET TO THIS PART QUICK) For each returned article, score them and see if the words associated with the article are synonyms with some descriptor terms.\n",
    "6. Suggest those additional terms to the user\n",
    "\n",
    "Examples:\n",
    "1. 'term=science[journal]+AND+breast+cancer+AND+2008[pdat]'\n",
    "2. '\"skin\"[MeSH Terms] OR \"rashes\"[MeSH Terms] OR \"purple\"[MeSH Terms]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "threshold = 90\n",
    "# Load SpaCy's language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stop words using NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(query):\n",
    "    doc = nlp(query)\n",
    "    # Filter out stop words and only keep nouns and adjectives\n",
    "    filtered_tokens = [token.lemma_ for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"] and token.lemma_.lower() not in stop_words]\n",
    "    # return \" \".join(filtered_tokens)\n",
    "    return filtered_tokens\n",
    "\n",
    "# Checks for each term in the query\n",
    "search_tokens = remove_stop_words('I have some purple rashes and cough.')\n",
    "\n",
    "\n",
    "def get_term_groupings_from_tokens(search_tokens, descriptors, concepts, terms):\n",
    "    expanded_search_tokens = {}\n",
    "    mandatory_terms = []\n",
    "    optional_terms = []\n",
    "    not_present_terms = []\n",
    "    # Checks whether query exists in descriptor layer\n",
    "    for token in search_tokens:\n",
    "        print('Token is:', token)\n",
    "        descriptor_key_match = process.extractOne(token.capitalize(), descriptors.keys(), score_cutoff=threshold, scorer=fuzz.ratio)\n",
    "        if descriptor_key_match:\n",
    "            print(f\"Best match in descriptors: {descriptor_key_match[0]} with score: {descriptor_key_match[1]}\")\n",
    "            print(descriptors[descriptor_key_match[0]])\n",
    "            expanded_search_tokens[token] = {\n",
    "                'descriptor' : descriptor_key_match[0]\n",
    "            }\n",
    "            # mandatory_terms.append (descriptor_key_match[0])\n",
    "            continue\n",
    "        # If not found, then move on to concept layer\n",
    "        # If found, then extract the preferred term and see whether it is in descriptor layer - put current term on 'or'\n",
    "        concept_key_match = process.extractOne(token.capitalize(), concepts.keys(), score_cutoff=threshold, scorer=fuzz.ratio)\n",
    "        if concept_key_match:\n",
    "            print(f\"Best match in concepts: {concept_key_match[0]} with score: {concept_key_match[1]}\")\n",
    "            print(concepts[concept_key_match[0]])\n",
    "            \n",
    "            # optional_terms.append(concept_key_match[0])\n",
    "            continue\n",
    "        # Else, move to term layer\n",
    "        # If found, then find preferred term and see whether it is in descriptor layer - put current term on 'or'\n",
    "        terms_key_match = process.extractOne(token.capitalize(), terms.keys(), score_cutoff=threshold, scorer=fuzz.ratio)\n",
    "        if terms_key_match:\n",
    "            print(f\"Best match in terms: {terms_key_match[0]} with score: {terms_key_match[1]}\")\n",
    "            print(terms[terms_key_match[0]])\n",
    "            optional_terms.append(terms_key_match[0])\n",
    "            continue\n",
    "        not_present_terms.append(token)\n",
    "\n",
    "# Add additional query suggestion for terms similar to what user is asking (spelling check query suggestion)\n",
    "query = ''\n",
    "\n",
    "first_term_inserted = False\n",
    "# If mesh term, then search mesh term only\n",
    "def build_query_from_tokens(mandatory_terms, optional_terms, not_in_mesh_terms):\n",
    "    query = ''\n",
    "    for mandatory_term in mandatory_terms:\n",
    "        if first_term_inserted:\n",
    "            query+=' AND '\n",
    "        query+='\"'+ mandatory_term + '\"' + '[MeSH Terms]'\n",
    "        first_term_inserted = True\n",
    "    # If got concept, expand by preferred concept (all in an 'OR')\n",
    "    # Also expand (maybe an 'OR') by the cocept\n",
    "    for optional_term in optional_terms:\n",
    "        if first_term_inserted:\n",
    "            query+=' OR '\n",
    "        query+='\"'+ optional_term + '\"' + '[All Fields]'\n",
    "    for not_present_term in not_in_mesh_terms:\n",
    "        if first_term_inserted:\n",
    "            query+=' OR '\n",
    "        query+='\"'+ not_present_term + '\"' + '[All Fields]'\n",
    "    # '\"skin\"[MeSH Terms] OR \"rashes\"[MeSH Terms] OR \"purple\"[MeSH Terms]'\n",
    "    # If exists in descriptor, move to next stage\n",
    "    # Create new query. For words totally not in either layer, put it on 'term search'\n",
    "    # Else, \n",
    "    query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code that extracts information straight from the internet is no longer relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def get_mesh_details(term):\n",
    "#     base_url = \"https://id.nlm.nih.gov/mesh/\"\n",
    "#     search_url = f\"{base_url}/lookup/descriptor?label={term}&match=contains\"\n",
    "\n",
    "#     response = requests.get(search_url)\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         return data\n",
    "#     else:\n",
    "#         print(f\"Error: {response.status_code}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# term = \"rash\"\n",
    "# mesh_details = get_mesh_details(term)\n",
    "\n",
    "# if mesh_details:\n",
    "#     for item in mesh_details:\n",
    "#         print(f\"Label: {item['label']}\")\n",
    "#         print(f\"ID: {item['resource']}\")\n",
    "#         print(f\"Tree Numbers: {item.get('treeNumbers', 'N/A')}\")\n",
    "#         print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper.pubmed_search as pubs\n",
    "# import random\n",
    "# import time\n",
    "# import json\n",
    "# import math\n",
    "\n",
    "# with open ('evaluation/BioASQ/BioASQ-training11b/training11b.json') as training_file:\n",
    "#     full_evaluation_data = json.load(training_file)\n",
    "\n",
    "# # Actually same as precision at k\n",
    "# # def simple_precision(retrieved_documents, relevant_documents):\n",
    "# #     # Convert to sets\n",
    "# #     set1 = set(retrieved_documents)\n",
    "# #     set2 = set(relevant_documents)\n",
    "# #     # Find the overlap (intersection)\n",
    "# #     overlap = set1 & set2\n",
    "# #     # If you want the size of the overlap\n",
    "# #     overlap_size = len(overlap)\n",
    "# #     try:\n",
    "# #         precision = overlap_size/len(retrieved_documents)*100\n",
    "# #     except Exception as e:\n",
    "# #         print('Error at measure_pak function:', e)        \n",
    "# #         precision = 0\n",
    "# #     return precision\n",
    "\n",
    "# def precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "#     k = min(k, len(retrieved_docs))  # Handle case where retrieved_docs < k\n",
    "#     relevant_in_top_k = [doc for doc in retrieved_docs[:k] if doc in relevant_docs]\n",
    "#     return 100*len(relevant_in_top_k) / k if k > 0 else 0.0\n",
    "\n",
    "\n",
    "# def average_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "#     k = min(k, len(retrieved_docs))  # Handle case where retrieved_docs < k\n",
    "#     if k == 0:\n",
    "#         return 0.0\n",
    "\n",
    "#     num_relevant = 0\n",
    "#     precision_sum = 0\n",
    "    \n",
    "#     for i in range(1, k + 1):\n",
    "#         if retrieved_docs[i - 1] in relevant_docs:\n",
    "#             num_relevant += 1\n",
    "#             precision_sum += num_relevant / i\n",
    "    \n",
    "#     # Use the smaller of k or total number of relevant documents\n",
    "#     return 100*precision_sum / min(len(relevant_docs), k) if num_relevant > 0 else 0.0\n",
    "\n",
    "# def ndcg_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "#     k = min(k, len(retrieved_docs))  # Handle case where retrieved_docs < k\n",
    "#     if k == 0:\n",
    "#         return 0.0\n",
    "\n",
    "#     def dcg(retrieved_docs, relevant_docs, k):\n",
    "#         dcg_value = 0.0\n",
    "#         for i in range(k):\n",
    "#             if retrieved_docs[i] in relevant_docs:\n",
    "#                 dcg_value += 1 / math.log2(i + 2)  # i + 2 because of 0-based indexing\n",
    "#         return dcg_value\n",
    "\n",
    "#     def idcg(relevant_docs, k):\n",
    "#         idcg_value = 0.0\n",
    "#         for i in range(min(len(relevant_docs), k)):\n",
    "#             idcg_value += 1 / math.log2(i + 2)\n",
    "#         return idcg_value\n",
    "    \n",
    "#     dcg_value = dcg(retrieved_docs, relevant_docs, k)\n",
    "#     idcg_value = idcg(relevant_docs, k)\n",
    "    \n",
    "#     return 100*dcg_value / idcg_value if idcg_value > 0 else 0.0\n",
    "\n",
    "\n",
    "# def extract_subset_of_evaluation_data(evaluation_data, fraction = 0.5, random_seed = 31):   \n",
    "#     random.seed(random_seed) \n",
    "#     subset = random.sample(evaluation_data, int(len(evaluation_data)*fraction))\n",
    "#     with open ('evaluation/BioASQ/subset.json', 'w') as outfile:\n",
    "#         json.dump(subset, outfile)\n",
    "#     print('Extracted', len(subset), 'documents out of', len(evaluation_data))\n",
    "\n",
    "# # Method to rank relevance of documents\n",
    "# def rank_initial_relevance(evaluation_set):\n",
    "#     # evaluation_set = []\n",
    "#     # if n_samples!=0:\n",
    "#     #     evaluation_set = random.sample(evaluation['questions'], n_samples)\n",
    "#     # else:        \n",
    "#     #     evaluation_set = evaluation['questions']\n",
    "#     # precisions = []\n",
    "#     retrieved_ids_list = []\n",
    "#     ground_truth_list = []\n",
    "#     # Run it through the system:\n",
    "#     for entry in evaluation_set:\n",
    "#         # When running the evaluation, there is a tendency for the internet to break so we need to continuously try to connnect\n",
    "#         rerun = True\n",
    "#         runs = 0\n",
    "#         while rerun:\n",
    "#             time.sleep(1) # rest for 1 second so as not to overwhelm PubMed's API\n",
    "#             # query the question through the database and extract the relevant documents\n",
    "#             try:\n",
    "#                 query_response = pubs.get_query_response(entry['body'], preprocess=False, articles_to_retrieve=10)\n",
    "#                 retrieved_ids = query_response['esearchresult']['idlist']\n",
    "#                 ground_truth = [x.split('/')[-1] for x in entry['documents']]    \n",
    "#                 retrieved_ids_list.append(retrieved_ids)    \n",
    "#                 ground_truth_list.append(ground_truth)   \n",
    "#                 rerun=False # once information is extracted, stop the rerun\n",
    "#             except Exception as e:\n",
    "#                 print('Error occured when querying pubmed:', e)\n",
    "#                 runs +=1\n",
    "#                 if runs > 5: # after 10 rounds of rerun due to errors, just move on\n",
    "#                     continue \n",
    "#                 rerun=True # if somehow information is not being able to be obtained, keep trying\n",
    "#     #     if len(retrieved_ids)!=0:\n",
    "#     #         precision = precision_at_k(retrieved_ids, ground_truth)\n",
    "#     #         print('Precision is:', precision)\n",
    "#     #     else:\n",
    "#     #         precision = 0\n",
    "#     #         print('No articles returned!')\n",
    "#     #     precisions.append(precision)\n",
    "#     # overall_precision = sum(precisions)/len(precisions)\n",
    "#     return retrieved_ids_list, ground_truth_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract MeSH Ontology Details into Usable Form (Already Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved function will return the preferred concept as well as term and descriptor objects to experiment with (**not used**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def find_term_in_mesh(xml_file, search_term):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()    \n",
    "#     term_objects = []\n",
    "#     descriptor_objects = []\n",
    "#     list_of_preferred_concept = []\n",
    "#     for descriptor_record in root.findall('.//DescriptorRecord'):\n",
    "#         descriptor_name = descriptor_record.find('.//DescriptorName/String').text\n",
    "\n",
    "#         # Identify the preferred concept within the descriptor\n",
    "#         preferred_concept = None\n",
    "#         for concept in descriptor_record.findall('.//Concept'):\n",
    "#             if concept.attrib.get('PreferredConceptYN', 'N') == 'Y':\n",
    "#                 preferred_concept = concept.find('.//ConceptName/String').text\n",
    "#                 description = 'Term '+concept.find('.//String').text+' has preferred concept: '+preferred_concept\n",
    "#                 list_of_preferred_concept.append(description)\n",
    "#                 break\n",
    "\n",
    "#         for concept in descriptor_record.findall('.//Concept'):\n",
    "#             concept_name = concept.find('.//ConceptName/String').text\n",
    "\n",
    "#             # Identify the preferred term within the concept\n",
    "#             preferred_term = None\n",
    "#             for term in concept.findall('.//Term'):\n",
    "#                 # if term.attrib.get('PreferredTermYN', 'N') == 'Y':  # for all terms within a concept, find the preferred term in the concept tree\n",
    "#                 if term.attrib.get('ConceptPreferredTermYN', 'N') == 'Y':  # for all terms within a concept, find the preferred term in the concept tree\n",
    "#                     preferred_term = term.find('.//String').text\n",
    "#                     break\n",
    "#             for term in concept.findall('.//Term'):\n",
    "#                 term_name = term.find('.//String').text\n",
    "#                 # if fuzz.ratio(search_term.lower(), term_name.lower()) > 80:\n",
    "#                 if search_term.lower() == term_name.lower():\n",
    "#                 # if search_term.lower() in term_name.lower():\n",
    "#                     print(f\"Found Term: {term_name}\")\n",
    "#                     print(f\"Descriptor: {descriptor_name}\")\n",
    "#                     print(f\"Preferred Concept: {preferred_concept}\")\n",
    "#                     print(f\"Concept: {concept_name}\")\n",
    "#                     print(f\"Preferred Term: {preferred_term}\")\n",
    "#                     print(\"-\" * 40)\n",
    "#                     term_objects.append(term)\n",
    "#                     descriptor_objects.append(descriptor_record)\n",
    "\n",
    "#     print(f\"Search complete for term '{search_term}'.\")\n",
    "#     return term_objects, descriptor_objects, list_of_preferred_concept\n",
    "\n",
    "# # Example usage\n",
    "# xml_file_path = 'record/desc2024.xml'  # Replace with your file path\n",
    "# search_term = 'rash'\n",
    "# a, b, c = find_term_in_mesh(xml_file_path, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code fragment for extracting annotation as well as scope (**not used**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "# #  Example function to extract descriptors and descriptions\n",
    "# def extract_descriptor_details(xml_file):\n",
    "#     # Parse the XML file\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "#     descriptors = []\n",
    "    \n",
    "#     for descriptor in root.findall(\".//DescriptorRecord\"):\n",
    "#         descriptor_name = descriptor.findtext(\"DescriptorName/String\")\n",
    "#         description = descriptor.findtext(\"ScopeNote\")  # ScopeNote usually contains the description\n",
    "#         scope_note = descriptor.findtext('.//ScopeNote')\n",
    "#         annotation = descriptor.findtext('.//Annotation')\n",
    "        \n",
    "#         descriptors.append({\n",
    "#             'DescriptorName': descriptor_name,\n",
    "#             'Description': description,\n",
    "#             'scope_note': scope_note,\n",
    "#             'annotation': annotation\n",
    "#         })\n",
    "    \n",
    "#     return descriptors\n",
    "\n",
    "# # Extract descriptors and descriptions\n",
    "# descriptors = extract_descriptor_details('record/desc2024.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building the index, all we need to know is what the preferred concept/term of a phrase is, as well as the related description. If got time, then only we build other things like narrowing and generalization (**final used function**, commented after we manage to retrieve proper index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "# from collections import defaultdict\n",
    "# import json\n",
    "\n",
    "# def create_mesh_preferred_index(xml_file):\n",
    "#     # Dictionary to hold the index\n",
    "#     descriptor_index = defaultdict(dict) # can include synonyms etc. in this\n",
    "#     concept_index = defaultdict(dict)\n",
    "#     term_index = defaultdict(dict)\n",
    "    \n",
    "#     # Parse the XML file\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # Traverse the XML and build the index\n",
    "#     for descriptor in root.findall('.//DescriptorRecord'):\n",
    "#         descriptor_name = descriptor.find('.//DescriptorName/String').text\n",
    "#         scope_note = descriptor.findtext('.//ScopeNote') or ''\n",
    "#         annotation = descriptor.findtext('.//Annotation') or ''\n",
    "#         combined_description = scope_note + annotation \n",
    "#         synonyms = [term.find(\"String\").text for term in descriptor.findall(\".//TermList/Term\")]\n",
    "#         # DESCRIPTOR LEVEL: The values for the index should be the synonyms and descriptions\n",
    "#         descriptor_index[descriptor_name] = {\n",
    "#             'Synonyms' : synonyms,\n",
    "#             'Annotation': annotation,\n",
    "#             'Scope Note': scope_note,\n",
    "#             'Combined Description': combined_description\n",
    "#         }\n",
    "#         # CONCEPT LEVEL: First need to find the preferred concept within this descriptor. Need to iterate through all\n",
    "#         preferred_concept = ''\n",
    "#         for concept in descriptor.findall('.//Concept'):\n",
    "#             if concept.attrib.get('PreferredConceptYN', 'N') == 'Y':\n",
    "#                 preferred_concept = concept.find('.//ConceptName/String').text\n",
    "#         # Then go to the next step by documenting all concepts and move to next loop\n",
    "#         for concept in descriptor.findall('.//Concept'):\n",
    "#             concept_name = concept.find('.//ConceptName/String').text\n",
    "#             concept_index[concept_name] = {\n",
    "#                 'preferred concept': preferred_concept,\n",
    "#                 'descriptor': descriptor_name\n",
    "#             }\n",
    "#             # TERM LEVEL: Just like for concepts, need to first find the preferred term\n",
    "#             preferred_term = ''\n",
    "#             for term in concept.findall('.//Term'):\n",
    "#                 # if term.attrib.get('PreferredConceptYN', 'N') == 'Y':\n",
    "#                 if term.attrib.get('ConceptPreferredTermYN', 'N') == 'Y':\n",
    "#                     preferred_term = term.find('.//String').text\n",
    "#             # Then, resume iteration\n",
    "#             for term in concept.findall('.//Term'):\n",
    "#                 term_name = term.find('.//String').text\n",
    "#                 term_index[term_name] = {\n",
    "#                     'preferred term': preferred_term,\n",
    "#                     'concept': concept_name,\n",
    "#                     'descriptor': descriptor_name\n",
    "#                 }\n",
    "#     return descriptor_index, concept_index, term_index\n",
    "\n",
    "# # Example usage\n",
    "# xml_file_path = 'record/desc2024.xml'\n",
    "# descriptors, concepts, terms = create_mesh_preferred_index(xml_file_path)\n",
    "\n",
    "# with open(\"helper/descriptors.json\", \"w\") as outfile: \n",
    "#     json.dump(descriptors, outfile)\n",
    "# with open(\"helper/concepts.json\", \"w\") as outfile: \n",
    "#     json.dump(concepts, outfile)\n",
    "# with open(\"helper/terms.json\", \"w\") as outfile: \n",
    "#     json.dump(terms, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRANCH 2: FIND PAPER LINKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: ['Jamie L Karch', 'Chiamaka L Okorie', 'Mayra B C Maymone', 'Melissa Laughter', 'Neelam A Vashi']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Define the paper's PubMed ID\n",
    "pubmed_id = '37936304'\n",
    "\n",
    "# Fetch paper details from PubMed\n",
    "url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pubmed_id}&retmode=xml'\n",
    "response = requests.get(url)\n",
    "tree = ElementTree.fromstring(response.content)\n",
    "\n",
    "# Extract authors\n",
    "authors = []\n",
    "for author in tree.findall(\".//Author\"):\n",
    "    last_name = author.find(\"LastName\").text if author.find(\"LastName\") is not None else \"\"\n",
    "    fore_name = author.find(\"ForeName\").text if author.find(\"ForeName\") is not None else \"\"\n",
    "    authors.append(f\"{fore_name} {last_name}\")\n",
    "\n",
    "print(\"Authors:\", authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles of other papers by the author: ['Neonatal nasal necrosis: Case series and brief review.', 'Vascular cutaneous manifestations of COVID-19 and RNA viral pathogens: a systematic review.', 'Subcutaneous Nodule With Poliosis: An Unusual Presentation of Melanoma Ex Blue Nevus.']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Step 1: Fetch the author's name from the original paper\n",
    "pubmed_id = '37936304'\n",
    "url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pubmed_id}&retmode=xml'\n",
    "response = requests.get(url)\n",
    "tree = ElementTree.fromstring(response.content)\n",
    "\n",
    "# Extract the first author (you can extract more as needed)\n",
    "author = tree.find(\".//Author\")\n",
    "last_name = author.find(\"LastName\").text\n",
    "initials = author.find(\"Initials\").text\n",
    "author_query = f\"{last_name} {initials}[Author]\"\n",
    "\n",
    "# Step 2: Search for other papers by this author\n",
    "search_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={author_query}&retmode=json\"\n",
    "search_response = requests.get(search_url).json()\n",
    "pubmed_ids = search_response.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "\n",
    "# Step 3: Fetch details of these papers\n",
    "if pubmed_ids:\n",
    "    id_string = \",\".join(pubmed_ids)\n",
    "    fetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={id_string}&retmode=xml\"\n",
    "    details_response = requests.get(fetch_url)\n",
    "    details_tree = ElementTree.fromstring(details_response.content)\n",
    "    \n",
    "    # Extract titles of these papers as an example\n",
    "    titles = [article.findtext(\"ArticleTitle\") for article in details_tree.findall(\".//Article\")]\n",
    "    print(\"Titles of other papers by the author:\", titles)\n",
    "else:\n",
    "    print(\"No other papers found for this author.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
